<!DOCTYPE html>
<html lang="zh-CN">

<head>
    <meta charset="UTF-8">
<meta name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">

    <meta name="author" content="Yoson Ling">





<title>南京大学《机器学习》课程笔记（ing） | Yoson&#39;s Blog</title>



    <link rel="icon" href="/ai.ico">




    <!-- stylesheets list from _config.yml -->
    
    <link rel="stylesheet" href="/css/style.css">
    



    <!-- scripts list from _config.yml -->
    
    <script src="/js/script.js"></script>
    
    <script src="/js/tocbot.min.js"></script>
    



    
    
        
    


<meta name="generator" content="Hexo 5.4.2"></head>

<body>
    <script>
        // this function is used to check current theme before page loaded.
        (() => {
            const currentTheme = window.localStorage && window.localStorage.getItem('theme') || '';
            const isDark = currentTheme === 'dark';
            const pagebody = document.getElementsByTagName('body')[0]
            if (isDark) {
                pagebody.classList.add('dark-theme');
                // mobile
                document.getElementById("mobile-toggle-theme").innerText = "· Dark"
            } else {
                pagebody.classList.remove('dark-theme');
                // mobile
                document.getElementById("mobile-toggle-theme").innerText = "· Light"
            }
        })();
    </script>

    <div class="wrapper">
        <header>
    <nav class="navbar">
        <div class="container">
            <div class="navbar-header header-logo"><a href="/">Yoson&#39;s Blog</a></div>
            <div class="menu navbar-right">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
                <input id="switch_default" type="checkbox" class="switch_default">
                <label for="switch_default" class="toggleBtn"></label>
            </div>
        </div>
    </nav>

    
    <nav class="navbar-mobile" id="nav-mobile">
        <div class="container">
            <div class="navbar-header">
                <div>
                    <a href="/">Yoson&#39;s Blog</a><a id="mobile-toggle-theme">·&nbsp;Light</a>
                </div>
                <div class="menu-toggle" onclick="mobileBtn()">&#9776; Menu</div>
            </div>
            <div class="menu" id="mobile-menu">
                
                    <a class="menu-item" href="/archives">Posts</a>
                
                    <a class="menu-item" href="/category">Categories</a>
                
                    <a class="menu-item" href="/tag">Tags</a>
                
                    <a class="menu-item" href="/about">About</a>
                
            </div>
        </div>
    </nav>

</header>
<script>
    var mobileBtn = function f() {
        var toggleMenu = document.getElementsByClassName("menu-toggle")[0];
        var mobileMenu = document.getElementById("mobile-menu");
        if(toggleMenu.classList.contains("active")){
           toggleMenu.classList.remove("active")
            mobileMenu.classList.remove("active")
        }else{
            toggleMenu.classList.add("active")
            mobileMenu.classList.add("active")
        }
    }
</script>
            <div class="main">
                <div class="container">
    
    
        <div class="post-toc">
    <div class="tocbot-list">
    </div>
    <div class="tocbot-list-menu">
        <a class="tocbot-toc-expand" onclick="expand_toc()">Expand all</a>
        <a onclick="go_top()">Back to top</a>
        <a onclick="go_bottom()">Go to bottom</a>
    </div>
</div>

<script>
    var tocbot_timer;
    var DEPTH_MAX = 6; // 为 6 时展开所有
    var tocbot_default_config = {
        tocSelector: '.tocbot-list',
        contentSelector: '.post-content',
        headingSelector: 'h1, h2, h3, h4, h5',
        orderedList: false,
        scrollSmooth: true,
        onClick: extend_click,
    };

    function extend_click() {
        clearTimeout(tocbot_timer);
        tocbot_timer = setTimeout(function() {
            tocbot.refresh(obj_merge(tocbot_default_config, {
                hasInnerContainers: true
            }));
        }, 420); // 这个值是由 tocbot 源码里定义的 scrollSmoothDuration 得来的
    }

    document.ready(function() {
        tocbot.init(obj_merge(tocbot_default_config, {
            collapseDepth: 1
        }));
    });

    function expand_toc() {
        var b = document.querySelector('.tocbot-toc-expand');
        var expanded = b.getAttribute('data-expanded');
        expanded ? b.removeAttribute('data-expanded') : b.setAttribute('data-expanded', true);
        tocbot.refresh(obj_merge(tocbot_default_config, {
            collapseDepth: expanded ? 1 : DEPTH_MAX
        }));
        b.innerText = expanded ? 'Expand all' : 'Collapse all';
    }

    function go_top() {
        window.scrollTo(0, 0);
    }

    function go_bottom() {
        window.scrollTo(0, document.body.scrollHeight);
    }

    function obj_merge(target, source) {
        for (var item in source) {
            if (source.hasOwnProperty(item)) {
                target[item] = source[item];
            }
        }
        return target;
    }
</script>
    

    
    <article class="post-wrap">
        <header class="post-header">
            <h1 class="post-title">南京大学《机器学习》课程笔记（ing）</h1>
            
                <div class="post-meta">
                    
                        Author: <a itemprop="author" rel="author" href="/">Yoson Ling</a>
                    

                    
                        <span class="post-time">
                        Date: <a href="#">二月 14, 2023&nbsp;&nbsp;00:00:00</a>
                        </span>
                    
                    
                        <span class="post-category">
                    Category:
                            
                                <a href="/categories/Learning/">Learning</a>
                            
                        </span>
                    
                </div>
            
        </header>

        <div class="post-content">
            <blockquote>
<p>本笔记中的图片均截取自课程PPT</p>
</blockquote>
<h1 id="机器学习概论"><a href="#机器学习概论" class="headerlink" title="机器学习概论"></a>机器学习概论</h1><h2 id="机器学习系统"><a href="#机器学习系统" class="headerlink" title="机器学习系统"></a>机器学习系统</h2><h3 id="通俗定义"><a href="#通俗定义" class="headerlink" title="通俗定义"></a>通俗定义</h3><p>任何通过数据训练的学习算法都属于机器学习</p>
<ul>
<li>线性回归（Linear Regression）</li>
<li>K-均值聚类（K-means）</li>
<li>主成分分析（Principal Component Analysis-PCA）</li>
<li>决策树（Decision Trees）和随机森林（Random Forest）</li>
<li>支持向量机（Support Vector Machines）</li>
<li>人工神经网络（Artificial Neural Networks）</li>
</ul>
<h3 id="学习系统"><a href="#学习系统" class="headerlink" title="学习系统"></a>学习系统</h3><p>模型空间（模型层面）+ 数据（数据层面）→ 学习算法（学习层面）→ 学得模型（学习结果）</p>
<p><strong>数据层面</strong></p>
<p>数据类型或特点：</p>
<ul>
<li>静态 vs. 动态</li>
<li>小数据 vs. 大数据</li>
<li>同质 vs. 异质</li>
<li>单态 vs. 多态</li>
<li>小类数 vs. 大类数</li>
<li>缺失 &amp; 带噪数据</li>
<li>高维数据 &amp; 非数值数据</li>
<li>……</li>
</ul>
<p><strong>模型层面</strong></p>
<p>形式：线性 / 非线性</p>
<p>体系：浅层 / 深度 / 递归</p>
<p><strong>学习层面</strong></p>
<p>经典 vs. 现代 vs. 混合</p>
<p>优化目标函数，在很大的模型空间找到目标函数（SGD 等学习算法）</p>
<p>经典学习方法：机械学习 / 归纳学习 / 类比学习 / 解释学习 / 决策树&amp;森林 / 贝叶斯分类器 / 聚类</p>
<p>现代学习方法：监督学习 / 弱监督学习 / 无监督学习 / 统计学习 / 集成学习 / 强化学习 / 深度学习</p>
<h2 id="系统建模和模型选择"><a href="#系统建模和模型选择" class="headerlink" title="系统建模和模型选择"></a>系统建模和模型选择</h2><p><strong>常规术语及标记</strong></p>
<ul>
<li>输入：x, x<del>i</del></li>
<li>权重：W, w<del>ij</del></li>
<li>输出：y, y(x, W)</li>
<li>目标：t, t<del>j</del></li>
<li>误差：E</li>
</ul>
<p><strong>数据集与数据集划分</strong></p>
<p>训练集，测试集，验证集（留出法，交叉验证）</p>
<p><strong>建模有关要素</strong></p>
<ul>
<li><p>模型 / 映射函数 f ( · ) 刻画（线性分类器？SVM？神经网络？）</p>
</li>
<li><p>确定目标 / 损失函数（平方损失？交叉熵？凸与非凸？）并优化获得模型</p>
</li>
<li><p>评测泛化性能（在未知样本上的预测能力）：欠拟合/过拟合</p>
</li>
</ul>
<p><strong>评价指标</strong></p>
<p>混淆矩阵（多分类，TP,FN,FP,TN:二分类），精度，错误率，查准率，查全率，F1度量</p>
<p><strong>ROC 曲线</strong></p>
<p><strong>不平衡数据集</strong></p>
<p><strong>模型选择</strong></p>
<p>正则化方法：约束选择空间</p>
<p><strong>吉洪诺夫正则化</strong></p>
<p>借助某个辅助非负泛函 / 模型实现解的稳定化</p>
<p>在泛函中嵌入了了解或问题的先验信息（如神经网络中权重衰减，在深度网络中用 Dropout）</p>
<p><strong>先验的重要性</strong></p>
<p>泛化（Generalization）= 数据（Data）+ 知识（Knowledge）</p>
<h2 id="统计学基本概念"><a href="#统计学基本概念" class="headerlink" title="统计学基本概念"></a>统计学基本概念</h2><p><strong>数据集的统计量</strong></p>
<ul>
<li>均值，中位数，众数</li>
<li>期望：概率加权和</li>
<li>方差，均方根</li>
<li>协方差（covariance）</li>
<li>协方差矩阵（covariance matrix）</li>
</ul>
<p><strong>距离度量函数</strong></p>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230305181503739.webp" alt="image-20230305181503739" style="zoom:50%;" />

<p><strong>高斯分布（正态分布）</strong></p>
<p><strong>概率</strong></p>
<p>从统计学角度，机器学习的目的是得到映射 x → y</p>
<ul>
<li>类的先验概率：p ( y = i )</li>
<li>样本的先验概率：p ( x )</li>
<li>类条件概率（似然）： p ( x | y = i )</li>
<li>后验概率：p ( y = i | x )</li>
</ul>
<p>从概率框架的角度对机器学习方法分类：</p>
<ul>
<li>生成式模型：估计类条件概率和类先验概率，用贝叶斯定理求后验概率</li>
<li>判别式模型：直接估计后验概率（使用判别函数，不假设概率模型，直接求一个把各类分开的边界）</li>
</ul>
<p><strong>朴素贝叶斯分类</strong></p>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230301165958734.webp" alt="image-20230301165958734" style="zoom:50%;" />

<h2 id="机器学习技术新进展"><a href="#机器学习技术新进展" class="headerlink" title="机器学习技术新进展"></a>机器学习技术新进展</h2><p><strong>新型机器学习技术</strong></p>
<ul>
<li>终身 / 连续学习（Lifelong / Continual Learning）</li>
<li>迁移学习和域适应（Transfer Learning &amp; Domain Adaption）</li>
<li>深度强化学习（Deep Reinforcement Learning）</li>
<li>对抗学习（Adversarial Learning）</li>
<li>元学习（Meta-Learning）</li>
<li>小样本学习（Few-shot Learning）</li>
<li>自监督学习（Self-supervised Learning）</li>
<li>联邦学习（Federated Learning）</li>
<li>……</li>
</ul>
<p><strong>新型机器学习发展趋势</strong></p>
<ul>
<li>模型层面<ul>
<li>大模型，大模型+领域知识，大模型+多模态信息/结构信息</li>
<li>小模型，模型蒸馏 / 量化（适配资源受限场景）</li>
<li>……</li>
</ul>
</li>
<li>优化层面<ul>
<li>在线 / 增量学习（Online / Incremental Learning）</li>
<li>分布 / 并行学习（Distributed / Parallel Learning）+ 异步优化</li>
<li>加速现有算法（Speedup existing algorithms）</li>
</ul>
</li>
<li>数据层面<ul>
<li>大数据，带噪声数据学习，多模态数据学习</li>
<li>小数据，总结或提炼数据，数据蒸馏</li>
</ul>
</li>
</ul>
<h1 id="概率学习"><a href="#概率学习" class="headerlink" title="概率学习"></a>概率学习</h1><h2 id="相关概念"><a href="#相关概念" class="headerlink" title="相关概念"></a>相关概念</h2><p><strong>符号和术语</strong></p>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230305160737157.webp" alt="image-20230305160737157" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230305160759643.webp" alt="image-20230305160759643" style="zoom:50%;" />

<p><strong>带约束的数学优化问题</strong></p>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230305160821582.webp" alt="image-20230305160821582" style="zoom:50%;" />

<p><strong>不带约束的数学优化问题</strong></p>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230305160846976.webp" alt="image-20230305160846976" style="zoom:50%;" />

<p><strong>凸函数与凹函数</strong></p>
<ul>
<li>凸函数</li>
</ul>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230305161027667.webp" alt="image-20230305161027667" style="zoom:50%;" />

<ul>
<li>凹函数</li>
</ul>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230305161048350.webp" alt="image-20230305161048350" style="zoom:50%;" />

<ul>
<li>判断函数的凹凸</li>
</ul>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230305161110550.webp" alt="image-20230305161110550" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230305161131271.webp" alt="image-20230305161131271" style="zoom:50%;" />

<p><strong>随机变量的期望</strong></p>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230305161213061.webp" alt="image-20230305161213061" style="zoom:50%;" />

<p><strong>Jensen’s inequality</strong></p>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230305161411615.webp" alt="image-20230305161411615" style="zoom:50%;" />

<p><strong>高斯分布/正态分布</strong></p>
<p>正态分布是在统计以及许多统计测试中最广泛应用的一类分布</p>
<p>正态分布是统计模式识别、计算机视觉和机器学习中使用最广泛的概率分布</p>
<ul>
<li>单变量高斯分布（Univariate Gaussian distribution）</li>
</ul>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230305161617946.webp" alt="image-20230305161617946" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230305161720811.webp" alt="image-20230305161720811" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230305161755356.webp" alt="image-20230305161755356" style="zoom:50%;" />

<ul>
<li>多变量高斯分布（Multivariate Gaussian distribution）</li>
</ul>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230305161821196.webp" alt="image-20230305161821196" style="zoom: 50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230305161921763.webp" alt="image-20230305161921763" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230305161958083.webp" alt="image-20230305161958083" style="zoom:50%;" />

<h2 id="高斯混合模型"><a href="#高斯混合模型" class="headerlink" title="高斯混合模型"></a>高斯混合模型</h2><h3 id="高斯混合模型（Gaussian-Mixture-Model-GMM）"><a href="#高斯混合模型（Gaussian-Mixture-Model-GMM）" class="headerlink" title="高斯混合模型（Gaussian Mixture Model, GMM）"></a>高斯混合模型（Gaussian Mixture Model, GMM）</h3><img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230305164455910.webp" alt="image-20230305164455910" style="zoom:50%;" />

<ul>
<li>概率密度函数</li>
</ul>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230305164409746.webp" alt="image-20230305164409746" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230305164520385.webp" alt="image-20230305164520385" style="zoom:50%;" />

<ul>
<li>图模型</li>
</ul>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230305164548996.webp" alt="image-20230305164548996" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230305164611515.webp" alt="image-20230305164611515" style="zoom:50%;" />

<h2 id="最大似然估计"><a href="#最大似然估计" class="headerlink" title="最大似然估计"></a>最大似然估计</h2><h3 id="最大似然估计（Maximum-likelihood-estimation，MLE）"><a href="#最大似然估计（Maximum-likelihood-estimation，MLE）" class="headerlink" title="最大似然估计（Maximum likelihood estimation，MLE）"></a>最大似然估计（Maximum likelihood estimation，MLE）</h3><img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230305171426184.webp" alt="image-20230305171426184" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230305171506580.webp" alt="image-20230305171506580" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230305171531820.webp" alt="image-20230305171531820" style="zoom:50%;" />

<h2 id="期望最大化算法"><a href="#期望最大化算法" class="headerlink" title="期望最大化算法"></a>期望最大化算法</h2><h3 id="EM-算法"><a href="#EM-算法" class="headerlink" title="EM 算法"></a>EM 算法</h3><p><strong>核心思想</strong></p>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230305172614172.webp" alt="image-20230305172614172" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230305172652574.webp" alt="image-20230305172652574" style="zoom:50%;" />

<p><strong>优化分析</strong></p>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230305172807483.webp" alt="image-20230305172807483" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230305172834223.webp" alt="image-20230305172834223" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230305180850321.webp" alt="image-20230305180850321" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230305180940898.webp" alt="image-20230305180940898" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230305181009838.webp" alt="image-20230305181009838" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230305181044018.webp" alt="image-20230305181044018" style="zoom:50%;" />

<p><strong>算法流程</strong></p>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230305181110353.webp" alt="image-20230305181110353" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230305181137967.webp" alt="image-20230305181137967" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230305181158298.webp" alt="image-20230305181158298" style="zoom:50%;" />

<h2 id="k-近邻分类器"><a href="#k-近邻分类器" class="headerlink" title="k-近邻分类器"></a>k-近邻分类器</h2><p><strong>算法流程</strong></p>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230305192643306.webp" alt="image-20230305192643306" style="zoom:50%;" />

<p><strong>k取值的影响</strong></p>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230305192729403.webp" alt="image-20230305192729403" style="zoom:50%;" />

<h2 id="最近邻分类器"><a href="#最近邻分类器" class="headerlink" title="最近邻分类器"></a>最近邻分类器</h2><p><strong>算法流程</strong></p>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230305194006168.webp" alt="image-20230305194006168" style="zoom:50%;" />

<p><strong>泛化错误率</strong></p>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230305194039700.webp" alt="image-20230305194039700" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230305194105010.webp" alt="image-20230305194105010" style="zoom:50%;" />

<h2 id="k-近邻回归"><a href="#k-近邻回归" class="headerlink" title="k-近邻回归"></a>k-近邻回归</h2><p><strong>算法流程</strong></p>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230305194143255.webp" alt="image-20230305194143255" style="zoom:50%;" />

<p><strong>近邻平滑</strong></p>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230305194221170.webp" alt="image-20230305194221170" style="zoom:50%;" />

<p><strong>讨论</strong></p>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230305194334078.webp" alt="image-20230305194334078" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230305194359508.webp" alt="image-20230305194359508" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230305194420413.webp" alt="image-20230305194420413" style="zoom:50%;" />

<p><strong>降低计算</strong></p>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230305194455953.webp" alt="image-20230305194455953" style="zoom:50%;" />



<h1 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h1><h2 id="聚类算法"><a href="#聚类算法" class="headerlink" title="聚类算法"></a>聚类算法</h2><p><strong>相关概念</strong></p>
<p>聚类（簇，类）：数据对象的集合，同类相似，不同类不相似</p>
<p>聚类算法（clustering algorithm）：根据给定的相似性评价标准，将一个数据集合划分成几个聚类</p>
<p>聚类依据：将样本看作是特征空间中的点，点与点的距离作为相似性评价标准</p>
<p>目的：潜在的自然分组结构/感兴趣的关系</p>
<p>聚类的关键：特征的选取或设计 + 距离度量函数的选择</p>
<p><strong>距离度量</strong></p>
<p><strong>聚类准则</strong></p>
<p>类的定义：距离小于阈值 / 聚类准则函数方法</p>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230301171457528.webp" alt="image-20230301171457528" style="zoom:50%;" />

<p><strong>聚类方法</strong></p>
<ul>
<li><p>基于试探的聚类搜索算法</p>
<ul>
<li>按最近邻规则的简单试探法</li>
</ul>
</li>
</ul>
<ul>
<li><p>系统聚类法</p>
<ul>
<li>按距离逐步分类（所有样本初始时视作独立的类别，由多到少的收敛过程）</li>
</ul>
</li>
</ul>
<ul>
<li><p>动态聚类法</p>
<ul>
<li><p>选取若干样本点作为聚类中心，再按照最小距离准则使样本点向各中心聚集，得到初始聚类</p>
</li>
<li><p>判断初始分类是否合理，若不合理则修改聚类，反复迭代直至合理为止</p>
</li>
</ul>
</li>
</ul>
<p><strong>K-means 算法</strong></p>
<p>K-means 算法是动态聚类法的代表算法</p>
<p>K-means 算法流程：</p>
<ol>
<li>选择聚类数量 k</li>
<li>随机选择 k 个样本点作为初始聚类中心 μ<del>1</del> μ<del>2</del> … μ<del>k</del></li>
<li>对每个样本点，计算其到 k 个聚类中心的距离，并将其分类到距离它最近的聚类中心所属聚类</li>
<li>对每个聚类，计算属于该聚类的所有样本点的均值，作为新的聚类中心</li>
<li>如果没有发生样本所属聚类改变（或达到最大迭代次数）则终止，否则返回第 3 步继续执行</li>
</ol>
<p>针对初始聚类中心的选取，有优化方案 K-means++ 算法</p>
<p><strong>聚类评价</strong></p>
<p>标签未知：紧密度，间隔度，戴维森堡丁指数，邓恩指数</p>
<p>标签已知：聚类准确率，兰德指数，调整兰德指数，互信息，归一化互信息</p>
<h2 id="前沿进展"><a href="#前沿进展" class="headerlink" title="前沿进展"></a>前沿进展</h2><h3 id="监督深度学习的成功与现实困境"><a href="#监督深度学习的成功与现实困境" class="headerlink" title="监督深度学习的成功与现实困境"></a>监督深度学习的成功与现实困境</h3><p>（监督）深度学习是如何成功的：</p>
<ul>
<li>预先定义许多视觉的概念及类别进行学习</li>
<li>为每个概念类别收集大量具有差异性的样本</li>
<li>对收集到的数据进行清洗和精细标注</li>
<li>采用多块 GPU 显卡训练几个小时甚至几天</li>
</ul>
<p>现实场景中存在的困境和问题：</p>
<ul>
<li>数据体量大</li>
<li>数据标注时间长</li>
<li>数据标注代价高</li>
<li>不能利用未标注的数据</li>
<li>监督信号可能使深度模型变得有偏</li>
</ul>
<h3 id="自监督学习"><a href="#自监督学习" class="headerlink" title="自监督学习"></a>自监督学习</h3><p>什么是自监督学习？（自监督预训练 + 下游任务迁移）</p>
<ul>
<li>无监督学习的一种形式，<strong>数据没有（人类标注的）监督信息</strong></li>
<li>需要<strong>定义一个前置（借口）任务</strong>让网络学习我们关心的事情</li>
<li>对于大部分前置任务，我们需要<strong>保留一部分数据</strong>，让网络学会预测</li>
<li>通过前置任务学习到的特征会被用到不同的<strong>下游任务</strong>（通常包含标注）</li>
</ul>
<p>自监督学习分类：前置任务学习，对比学习，非对比学习</p>
<h4 id="前置任务学习"><a href="#前置任务学习" class="headerlink" title="前置任务学习"></a>前置任务学习</h4><p><strong>生成式方法——图像着色</strong></p>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230301215532176.webp" alt="image-20230301215532176" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230301215601541.webp" alt="image-20230301215601541" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230301215624475.webp" alt="image-20230301215624475" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230301215646345.webp" alt="image-20230301215646345" style="zoom:50%;" />

<p><strong>生成式方法——图像修复</strong></p>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230301215726738.webp" alt="image-20230301215726738" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230301215744738.webp" alt="image-20230301215744738" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230301230539112.webp" alt="image-20230301230539112" style="zoom:50%;" />

<p><strong>判别式方法——图像拼图</strong></p>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230301230824904.webp" alt="image-20230301230824904" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230301230846766.webp" alt="image-20230301230846766" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230301230907470.webp" alt="image-20230301230907470" style="zoom:50%;" />

<h4 id="对比学习"><a href="#对比学习" class="headerlink" title="对比学习"></a>对比学习</h4><img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230301231029180.webp" alt="image-20230301231029180" style="zoom:50%;" />

<p><strong>MoCo</strong></p>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230301231102221.webp" alt="image-20230301231102221" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230301231124723.webp" alt="image-20230301231124723" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230301231151658.webp" alt="image-20230301231151658" style="zoom:50%;" />

<p><strong>SimCLR</strong></p>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230301231419746.webp" alt="image-20230301231419746" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230301231441420.webp" alt="image-20230301231441420" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230301231502418.webp" alt="image-20230301231502418" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230301231536641.webp" alt="image-20230301231536641" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230301231857690.webp" alt="image-20230301231857690" style="zoom:50%;" />

<h4 id="非对比学习"><a href="#非对比学习" class="headerlink" title="非对比学习"></a>非对比学习</h4><p><strong>聚类方法——DeepCluster</strong></p>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230301231650616.webp" alt="image-20230301231650616" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230301231709428.webp" alt="image-20230301231709428" style="zoom:50%;" />

<p><strong>聚类方法——SwAV</strong></p>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230301231734407.webp" alt="image-20230301231734407" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230301231755524.webp" alt="image-20230301231755524" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230301231827445.webp" alt="image-20230301231827445" style="zoom:50%;" />

<h2 id="作业一：K-means"><a href="#作业一：K-means" class="headerlink" title="作业一：K-means"></a>作业一：K-means</h2><p>要求：实现 K-means 聚类并使用其进行无监督图像分割</p>
<p><strong>代码展示</strong>：<a target="_blank" rel="noopener" href="https://github.com/Ling-Yuchen/ML-hw/blob/main/kmeans.py">https://github.com/Ling-Yuchen/ML-hw/blob/main/kmeans.py</a></p>
<h3 id="实验报告"><a href="#实验报告" class="headerlink" title="实验报告"></a>实验报告</h3><h4 id="算法原理"><a href="#算法原理" class="headerlink" title="算法原理"></a>算法原理</h4><p>略</p>
<h4 id="代码说明"><a href="#代码说明" class="headerlink" title="代码说明"></a>代码说明</h4><p>在本次实验中使用了两种方法来实现 K-means 聚类算法，一是参考 <a target="_blank" rel="noopener" href="https://blog.csdn.net/lanshi00/article/details/104109963">https://blog.csdn.net/lanshi00/article/details/104109963</a> 调用已有封装好的 <code>cv2.kmeans()</code>，二是参考 <a target="_blank" rel="noopener" href="https://github.com/Daya-Jin/ML_for_learner/blob/master/cluster/KMeans.ipynb">https://github.com/Daya-Jin/ML_for_learner/blob/master/cluster/KMeans.ipynb</a> 使用 <code>numpy</code> 等库手动实现算法流程，两者均使用 <code>cv2.imshow()</code> 作为可视化实现方案</p>
<ul>
<li><p><strong>方法一</strong></p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">compactness, clusters, centers = cv2.kmeans(</span><br><span class="line">    data=data,</span><br><span class="line">    K=k,</span><br><span class="line">    bestLabels=<span class="literal">None</span>,</span><br><span class="line">    criteria=(</span><br><span class="line">        cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER,  <span class="comment"># type</span></span><br><span class="line">        <span class="number">10</span>,  <span class="comment"># max_iter</span></span><br><span class="line">        <span class="number">1.0</span>  <span class="comment"># epsilon</span></span><br><span class="line">    ),  <span class="comment"># define condition of ending</span></span><br><span class="line">    attempts=<span class="number">10</span>,</span><br><span class="line">    flags=cv2.KMEANS_PP_CENTERS  <span class="comment"># set initial centers</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>  关键参数：</p>
<ul>
<li>criteria：规定了迭代的终止条件</li>
<li>flags：规定了初始中心的生成方式</li>
<li>k：规定了目标类簇的个数</li>
</ul>
</li>
<li><p><strong>方法二</strong></p>
<p>  每次迭代的核心代码如下：</p>
  <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">epoch += <span class="number">1</span></span><br><span class="line"><span class="comment"># calculate distance from each sample to each center</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">	dist[:, i] = np.linalg.norm(data - cent_cur[i], axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># each sample belongs to cluster of the nearest center</span></span><br><span class="line">clusters = np.argmin(dist, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">cent_pre = deepcopy(cent_cur)</span><br><span class="line"><span class="comment"># calculate mean coordinate on each cluster, update center</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">	cent_cur[i] = np.mean(data[clusters == i], axis=<span class="number">0</span>)</span><br><span class="line">cent_move = np.linalg.norm(cent_cur - cent_pre)</span><br></pre></td></tr></table></figure>

<p>  核心代码说明：</p>
<ul>
<li><p><code>data</code> 表示经过处理的输入图像数据，其形状为  (65536, 1)（经过灰度处理）或 (65536, 3)（未经过灰度处理）（输入的图像被固定 resize 为 256*256，固共有 65536 个样本点）</p>
</li>
<li><p>每次迭代需要递增迭代次数，并计算各聚类中心的改变量，若迭代次数达到最大值或改变量低于设定阈值，则终止迭代</p>
</li>
<li><p>计算样本点到各聚类中心的距离，使用 <code>np.linalg.norm()</code>，默认为二范数，即计算欧拉距离，计算结果保存在一个形状为 (65536, k) 的距离矩阵中</p>
</li>
<li><p>依据距离矩阵对样本点进行分类，使用 <code>np.argmin()</code>，返回一个最小值索引向量，表示本次迭代产生的聚类结果</p>
</li>
</ul>
</li>
</ul>
<h4 id="可视化实验结果分析"><a href="#可视化实验结果分析" class="headerlink" title="可视化实验结果分析"></a>可视化实验结果分析</h4><ul>
<li>高斯模糊对实验结果影响的讨论<ul>
<li>在预处理步骤中增加了高斯模糊可以明显减少聚类结果中边界处的噪声点，使得分类更加准确，图像分割结果更加美观</li>
</ul>
</li>
</ul>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230301084839502.webp" alt="image-20230301084839502" style="zoom:40%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230301084720367.webp" alt="image-20230301084720367" style="zoom:50%;" />

<ul>
<li>使用灰度图对实验结果影响的讨论<ul>
<li>在主体色调相对统一的情况下，在原图上和在灰度图上进行处理最后得到的分割结果基本相同（第一组对比图），使用灰度图可以在保证分割效果的情况下使算法的计算量缩减为 1/3</li>
<li>在色调有明显区分的情况下，使用灰度图会因为损失了一部分信息而得到偏离预期的分割结果（第二组对比图）</li>
</ul>
</li>
</ul>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230301085825192.webp" alt="image-20230301085825192" style="zoom:40%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230301085946910.webp" alt="image-20230301085946910" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230301185012430.webp" alt="image-20230301185012430" style="zoom:40%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230301185127501.webp" alt="image-20230301185127501" style="zoom:50%;" />

<ul>
<li>更多分割结果展示</li>
</ul>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230301090716162.webp" alt="image-20230301090716162" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230301090905016.webp" alt="image-20230301090905016" style="zoom:50%;" />



<h1 id="支持向量机"><a href="#支持向量机" class="headerlink" title="支持向量机"></a>支持向量机</h1><h2 id="感知机：线性超平面"><a href="#感知机：线性超平面" class="headerlink" title="感知机：线性超平面"></a>感知机：线性超平面</h2><p>二分类问题可以看作在线性空间上对类别进行划分的任务</p>
<p><em><strong>w</strong></em>^T^<em><strong>x</strong></em> + <em>b</em> = 0：划分超平面的线性方程，其中 <em><strong>w</strong></em> 为法向量，决定超平面的方向；<em>b</em> 为位移项，决定了超平面与原点之间的距离</p>
<p>强假设：处于“正中间”的超平面具有更好的鲁棒性和泛化能力</p>
<h2 id="线性支持向量机（LSVM）"><a href="#线性支持向量机（LSVM）" class="headerlink" title="线性支持向量机（LSVM）"></a>线性支持向量机（LSVM）</h2><h3 id="间隔与支持向量"><a href="#间隔与支持向量" class="headerlink" title="间隔与支持向量"></a>间隔与支持向量</h3><p>间隔（margin）：每个样本点到超平面的垂直距离</p>
<p>超平面的优化目标：最大化所有训练样本的最小间隔</p>
<p>支持向量（support vector）：具有最小间隔的样本点</p>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230303225744792.webp" alt="image-20230303225744792" style="zoom:50%;" />

<h3 id="计算-Margin"><a href="#计算-Margin" class="headerlink" title="计算 Margin"></a>计算 Margin</h3><img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230303230248914.webp" alt="image-20230303230248914" style="zoom:50%;" />

<h3 id="分类与评价"><a href="#分类与评价" class="headerlink" title="分类与评价"></a>分类与评价</h3><p>分类方式：<em>f(x)</em> &gt; 0 → 正类；<em>f(x)</em> &lt; 0 → 负类</p>
<p>判断预测的对错：对样本点 *x<del>i</del>*，用 <em>y<del>i</del></em> ∈ {-1, 1} 作为其正负类的标注，则有</p>
<ul>
<li><em>y<del>i</del> f(<strong>x</strong><del>i</del>)</em> &gt; 0 → 预测正确；<em>y<del>i</del> f(<strong>x</strong><del>i</del>)</em> &lt; 0 → 预测错误</li>
<li>假设能够将样本点完全分开，并且 <em>|y<del>i</del>|</em> = 1，则有 <em>y<del>i</del> f(<strong>x</strong><del>i</del>)</em> = <em>|f(<strong>x</strong><del>i</del>)</em>|</li>
</ul>
<h3 id="SVM-的形式化描述"><a href="#SVM-的形式化描述" class="headerlink" title="SVM 的形式化描述"></a>SVM 的形式化描述</h3><img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230303231456468.webp" alt="image-20230303231456468" style="zoom:50%;" />

<p><strong>换个角度看问题以简化目标</strong></p>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230303231756077.webp" alt="image-20230303231756077" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230303234051787.webp" alt="image-20230303234051787" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230303234116837.webp" alt="image-20230303234116837" style="zoom:50%;" />

<h3 id="拉格朗日乘子法"><a href="#拉格朗日乘子法" class="headerlink" title="拉格朗日乘子法"></a>拉格朗日乘子法</h3><img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230303234236689.webp" alt="image-20230303234236689" style="zoom:50%;" />

<h3 id="KKT-条件"><a href="#KKT-条件" class="headerlink" title="KKT 条件"></a>KKT 条件</h3><img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230303234341963.webp" alt="image-20230303234341963" style="zoom:50%;" />

<h3 id="SVM-的对偶形式"><a href="#SVM-的对偶形式" class="headerlink" title="SVM 的对偶形式"></a>SVM 的对偶形式</h3><img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230303234520031.webp" alt="image-20230303234520031" style="zoom:50%;" />

<h3 id="Soft-margin"><a href="#Soft-margin" class="headerlink" title="Soft margin"></a>Soft margin</h3><img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230304091100595.webp" alt="image-20230304091100595" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230304091129110.webp" alt="image-20230304091129110" style="zoom:50%;" />

<p><strong>对犯错误进行惩罚</strong></p>
<ul>
<li>在原始空间内：</li>
</ul>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230304091334360.webp" alt="image-20230304091334360" style="zoom:50%;" />

<p>其中，<em>C</em> &gt; 0，是一个正则化参数；<em>ξ<del>i</del></em> 是代价（要最小化代价函数）；<em><strong>w^T^w</strong></em> 是正则项，对分类器进行限制，限制模型的复杂度（还是最大化间隔）</p>
<ul>
<li>在对偶空间内：</li>
</ul>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230304091925800.webp" alt="image-20230304091925800" style="zoom:50%;" />

<p><em>对偶形式仅依赖于样本的内积</em></p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul>
<li>分类器是一个分离超平面（separating hyperplane）</li>
<li>最重要的训练样本是“支持向量”，它们定义了超平面，其他样本被忽略。二次优化算法可以识别哪些样本是具有非零拉格朗日乘子 <em>α<del>i</del></em> 的支持向量</li>
<li>对偶问题中，训练样本只以内积的形式出现</li>
</ul>
<h2 id="非线性支持向量机"><a href="#非线性支持向量机" class="headerlink" title="非线性支持向量机"></a>非线性支持向量机</h2><img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230304092349504.webp" alt="image-20230304092349504" style="zoom:50%;" />

<h3 id="基本思想"><a href="#基本思想" class="headerlink" title="基本思想"></a>基本思想</h3><p>将样本从原始空间映射到一个更高维的特征空间，使样本在这个特征空间内线性可分（特征空间映射）</p>
<p>可以证明，如果原始空间是有限维，那么一定存在一个高维特征空间使得样本线性可分</p>
<h3 id="通过内积联系线性与非线性"><a href="#通过内积联系线性与非线性" class="headerlink" title="通过内积联系线性与非线性"></a>通过内积联系线性与非线性</h3><img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230304092706511.webp" alt="image-20230304092706511" style="zoom:50%;" />

<p>如图，两个二维样本在非线性函数作用下，得到六维特征空间的内积形式</p>
<h3 id="Kernel-trick"><a href="#Kernel-trick" class="headerlink" title="Kernel trick"></a>Kernel trick</h3><img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230304093033299.webp" alt="image-20230304093033299" style="zoom:50%;" />

<p><strong>限制条件</strong></p>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230304093246829.webp" alt="image-20230304093246829" style="zoom:50%;" />

<h3 id="核支持向量机"><a href="#核支持向量机" class="headerlink" title="核支持向量机"></a>核支持向量机</h3><img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230304093537351.webp" alt="image-20230304093537351" style="zoom:50%;" />

<p><strong>核函数的种类</strong></p>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230304093612535.webp" alt="image-20230304093612535" style="zoom:50%;" />

<p><strong>非线性核的例子</strong></p>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230304093733774.webp" alt="image-20230304093733774" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230304093759599.webp" alt="image-20230304093759599" style="zoom:50%;" />

<p><strong>关于超参数</strong></p>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230304093944568.webp" alt="image-20230304093944568" style="zoom:50%;" />

<h2 id="多类支持向量机"><a href="#多类支持向量机" class="headerlink" title="多类支持向量机"></a>多类支持向量机</h2><p>思路：转化为二分类问题</p>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230304094122394.webp" alt="image-20230304094122394" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230304094144147.webp" alt="image-20230304094144147" style="zoom:50%;" />

<p><strong>解决方案</strong></p>
<ul>
<li>Crammer-Singer 方法：<a target="_blank" rel="noopener" href="http://jmlr.org/papers/v2/crammer01a.html">http://jmlr.org/papers/v2/crammer01a.html</a></li>
<li>DAGSVM：<a target="_blank" rel="noopener" href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/dagsvm.pdf">https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/dagsvm.pdf</a></li>
<li>ECOC：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/cs/9501101.pdf">https://arxiv.org/pdf/cs/9501101.pdf</a></li>
</ul>
<h2 id="支持向量机的实现"><a href="#支持向量机的实现" class="headerlink" title="支持向量机的实现"></a>支持向量机的实现</h2><p>SVM Website：<a target="_blank" rel="noopener" href="http://www.kernel-machines.org/">http://www.kernel-machines.org/</a></p>
<p>代表性实现 LIBSVM：有效且出名，实现了 muti-class classification，nu-SVM，one-class SVM 等，且有很多 Java、Python 等接口</p>
<h2 id="支持向量机的启发"><a href="#支持向量机的启发" class="headerlink" title="支持向量机的启发"></a>支持向量机的启发</h2><img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230304094726817.webp" alt="image-20230304094726817" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230304094746716.webp" alt="image-20230304094746716" style="zoom:50%;" />

<h2 id="前沿进展——持续学习"><a href="#前沿进展——持续学习" class="headerlink" title="前沿进展——持续学习"></a>前沿进展——持续学习</h2><h3 id="任务定义"><a href="#任务定义" class="headerlink" title="任务定义"></a>任务定义</h3><img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230305150147185.webp" alt="image-20230305150147185" style="zoom:50%;" />

<h3 id="灾难性遗忘"><a href="#灾难性遗忘" class="headerlink" title="灾难性遗忘"></a>灾难性遗忘</h3><img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230305151055008.webp" alt="image-20230305151055008" style="zoom:50%;" />

<h3 id="场景设定"><a href="#场景设定" class="headerlink" title="场景设定"></a>场景设定</h3><img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230305151157804.webp" alt="image-20230305151157804" style="zoom:50%;" />

<ul>
<li><p><strong>任务增量学习</strong></p>
<ul>
<li><p>不同时刻到来的数据分属于不同的任务，在一个时间段内我们可以获得当前任务的全部数据，并且假设测试推理阶段，任务的编号是可知的</p>
</li>
<li><p>因为测试阶段任务编号可知，所以训练阶段不同任务的输出相互独立，即最终学到的模型是多头的（multi-head），为每个任务提供单独的输出层</p>
</li>
</ul>
</li>
<li><p><strong>类别增量学习</strong></p>
<ul>
<li>不同时刻到来的数据属于同一类型任务的不同类别，随着训练的进行逐步增加输出类别，即<strong>类别空间是逐渐扩充的</strong></li>
<li>不同于任务增量学习场景，类别增量学习场景在测试阶段不知道任务的编号（训练阶段知道任务编号），比任务增量场景更具挑战性</li>
</ul>
</li>
<li><p><strong>域增量学习</strong></p>
<ul>
<li>不同时刻到来的数据集属于不同任务，但任务之间的<strong>类别空间是相同的</strong>，只是数据的<strong>分布（域）发生了变化</strong>，前后两个任务的数据不再满足独立同分布假设</li>
</ul>
</li>
</ul>
<h3 id="持续学习设置"><a href="#持续学习设置" class="headerlink" title="持续学习设置"></a>持续学习设置</h3><h4 id="常用数据集"><a href="#常用数据集" class="headerlink" title="常用数据集"></a>常用数据集</h4><img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230305153343052.webp" alt="image-20230305153343052" style="zoom:50%;" />

<h4 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h4><img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230305153315978.webp" alt="image-20230305153315978" style="zoom:50%;" />

<h3 id="持续学习方法分类"><a href="#持续学习方法分类" class="headerlink" title="持续学习方法分类"></a>持续学习方法分类</h3><img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230305153209815.webp" alt="image-20230305153209815" style="zoom:50%;" />

<h3 id="大作业（Optional）"><a href="#大作业（Optional）" class="headerlink" title="大作业（Optional）"></a>大作业（Optional）</h3><img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230305153519164.webp" alt="image-20230305153519164" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230305153546632.webp" alt="image-20230305153546632" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230305153607991.webp" alt="image-20230305153607991" style="zoom:50%;" />



<h1 id="神经元和感知机"><a href="#神经元和感知机" class="headerlink" title="神经元和感知机"></a>神经元和感知机</h1><h2 id="脑和神经元"><a href="#脑和神经元" class="headerlink" title="脑和神经元"></a>脑和神经元</h2><p><strong>MP神经元基本结构</strong></p>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230318203358136.webp" alt="image-20230318203358136" style="zoom:50%;" />

<h2 id="感知机和感知机学习"><a href="#感知机和感知机学习" class="headerlink" title="感知机和感知机学习"></a>感知机和感知机学习</h2><h2 id="线性可分性"><a href="#线性可分性" class="headerlink" title="线性可分性"></a>线性可分性</h2><h1 id="神经元网络"><a href="#神经元网络" class="headerlink" title="神经元网络"></a>神经元网络</h1><h2 id="多层感知机"><a href="#多层感知机" class="headerlink" title="多层感知机"></a>多层感知机</h2><h2 id="自动编码器"><a href="#自动编码器" class="headerlink" title="自动编码器"></a>自动编码器</h2><h2 id="径向基网络"><a href="#径向基网络" class="headerlink" title="径向基网络"></a>径向基网络</h2><h1 id="树学习"><a href="#树学习" class="headerlink" title="树学习"></a>树学习</h1><h2 id="概念学习和变型空间"><a href="#概念学习和变型空间" class="headerlink" title="概念学习和变型空间"></a>概念学习和变型空间</h2><p><strong>概念学习</strong></p>
<p>给定样例集合，以及每个样例是否属于某个概念，<strong>自动</strong>地推断出该概念的一般定义</p>
<p><strong>概念学习任务</strong></p>
<p>实例集合：用若干属性表示</p>
<p>目标概念$$c$$：定义在实例集上的布尔函数 $$c:X\rightarrow {0,1}$$</p>
<p>训练样例：正例$$(c(x)=1)$$，反例$$(c(x)=0)$$</p>
<p>假设集$$H$$：每个假设$$h$$表示$$X$$上定义的布尔函数$$h:X\rightarrow {0,1}$$</p>
<p>概念学习任务：寻找一个假设$$h$$，使对于$$X$$中说有$$x$$，$$h(x)=c(x)$$</p>
<p><strong>实例空间和假设数</strong></p>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230318210112281.webp" alt="image-20230318210112281" style="zoom:50%;" />

<p><strong>Find-S:寻找极大特殊假设</strong></p>
<blockquote>
<p>对以属性合取式表示的假设空间，输出与正例一致的最特殊的假设</p>
</blockquote>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230318210312848.webp" alt="image-20230318210312848" style="zoom:50%;" />

<p><strong>变形空间</strong></p>
<p>假设h与训练样例D集合一致：$$Consistent(h,D)\equiv(\forall&lt;x,c(x)&gt;\in D)\space h(x)=c(x)$$</p>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331152536464.webp" alt="image-20230331152536464" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331152459947.webp" alt="image-20230331152459947" style="zoom:50%;" />

<p><strong>正例和反例的作用</strong></p>
<ul>
<li>正例用于S泛化，搜索S集合</li>
<li>反例用于G特化，缩小G集合</li>
</ul>
<p><strong>候选消除算法</strong></p>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331151459518.webp" alt="image-20230331151459518" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331151529211.webp" alt="image-20230331151529211" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331151556095.webp" alt="image-20230331151556095" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331151621870.webp" alt="image-20230331151621870" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331151642353.webp" alt="image-20230331151642353" style="zoom:50%;" />

<h2 id="归纳偏置"><a href="#归纳偏置" class="headerlink" title="归纳偏置"></a>归纳偏置</h2><blockquote>
<p>原假设空间是由合取式（有偏）表示，而真实空间是由析取式表示</p>
</blockquote>
<p><strong>归纳偏置</strong></p>
<p>归纳学习必须给定某种形式的预先假定（归纳偏置）</p>
<p>核心：学习器从训练样例中泛化并推断新实例分类过程中所采用的策略</p>
<p>精确定义：</p>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331152226050.webp" alt="image-20230331152226050" style="zoom:50%;" />

<h2 id="决策树算法"><a href="#决策树算法" class="headerlink" title="决策树算法"></a>决策树算法</h2><h3 id="决策树学习"><a href="#决策树学习" class="headerlink" title="决策树学习"></a>决策树学习</h3><p>决策树学习特点：</p>
<ul>
<li><p>实例由“属性-值”对表示，应用广泛</p>
</li>
<li><p>目标函数具有离散的输出值</p>
</li>
<li><p>健壮性好（允许少量错误和缺值实例）</p>
</li>
<li><p>能够学习析取表达式</p>
</li>
</ul>
<p>决策树学习算法：</p>
<ul>
<li>ID3，Assistant，C4.5</li>
<li>搜索一个完整表示的假设空间，表示为多个 if-then 规则</li>
</ul>
<p>决策树学习归纳偏置：</p>
<ul>
<li>优先选择较小的树</li>
</ul>
<p>决策树表示样例：</p>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331104838643.webp" alt="image-20230331104838643" style="zoom:40%;" />

<h3 id="问题设置"><a href="#问题设置" class="headerlink" title="问题设置"></a>问题设置</h3><img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331104941643.webp" alt="image-20230331104941643" style="zoom:50%;" />

<h3 id="决策树学习的假设空间搜索"><a href="#决策树学习的假设空间搜索" class="headerlink" title="决策树学习的假设空间搜索"></a>决策树学习的假设空间搜索</h3><ul>
<li>从一个假设空间中搜索一个正确拟合训练样例的假设</li>
<li>搜索的假设空间为可能的决策树集合</li>
<li>使用从简单到复杂的爬山算法遍历假设空间：从空的树开始，逐步考虑更加复杂的假设（引导爬山搜索的评估函数是信息增益度量）</li>
</ul>
<h3 id="用于学习布尔函数的ID3算法"><a href="#用于学习布尔函数的ID3算法" class="headerlink" title="用于学习布尔函数的ID3算法"></a>用于学习布尔函数的ID3算法</h3><img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331105509995.webp" alt="image-20230331105509995" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331105534888.webp" alt="image-20230331105534888" style="zoom:50%;" />

<h3 id="最佳属性的选择"><a href="#最佳属性的选择" class="headerlink" title="最佳属性的选择"></a>最佳属性的选择</h3><ul>
<li>衡量给定的属性区分训练样例的能力：信息增益（information gain）</li>
<li>信息的度量：熵（entropy），刻画了样例集的纯度</li>
<li>目标属性为布尔值的样例集S的熵：$$Entropy(S)=-p_+ log_2 p_+ -p_- log_2 p_-$$ </li>
</ul>
<h3 id="信息增益"><a href="#信息增益" class="headerlink" title="信息增益"></a>信息增益</h3><img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331110352074.webp" alt="image-20230331110352074" style="zoom:50%;" />

<h3 id="ID3算法特点"><a href="#ID3算法特点" class="headerlink" title="ID3算法特点"></a>ID3算法特点</h3><ul>
<li>假设空间：包含所有的决策树</li>
<li>遍历过程：仅维持单一的当前假设（不同于变形空间候选消除算法）</li>
<li>回溯：不进行回溯（局部最优）</li>
<li>基于统计：对错误样例不敏感；不适用于增量处理</li>
</ul>
<h3 id="决策树学习中的归纳偏置"><a href="#决策树学习中的归纳偏置" class="headerlink" title="决策树学习中的归纳偏置"></a>决策树学习中的归纳偏置</h3><img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331110825102.webp" alt="image-20230331110825102" style="zoom:50%;" />

<h3 id="奥卡姆剃刀原理"><a href="#奥卡姆剃刀原理" class="headerlink" title="奥卡姆剃刀原理"></a>奥卡姆剃刀原理</h3><img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331110928813.webp" alt="image-20230331110928813" style="zoom:50%;" />

<h3 id="代表性树算法对比"><a href="#代表性树算法对比" class="headerlink" title="代表性树算法对比"></a>代表性树算法对比</h3><img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331111027678.webp" alt="image-20230331111027678" style="zoom:50%;" />

<h3 id="C4-5算法"><a href="#C4-5算法" class="headerlink" title="C4.5算法"></a>C4.5算法</h3><img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331113021181.webp" alt="image-20230331113021181" style="zoom:50%;" />

<h3 id="CART算法"><a href="#CART算法" class="headerlink" title="CART算法"></a>CART算法</h3><img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331113048888.webp" alt="image-20230331113048888" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331113115865.webp" alt="image-20230331113115865" style="zoom:50%;" />

<h3 id="基尼系数"><a href="#基尼系数" class="headerlink" title="基尼系数"></a>基尼系数</h3><img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331112633813.webp" alt="image-20230331112633813" style="zoom:50%;" />

<h3 id="连续值处理"><a href="#连续值处理" class="headerlink" title="连续值处理"></a>连续值处理</h3><p><strong>连续的特征离散化</strong></p>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331112713592.webp" alt="image-20230331112713592" style="zoom:50%;" />

<h3 id="离散值处理"><a href="#离散值处理" class="headerlink" title="离散值处理"></a>离散值处理</h3><img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331112755659.webp" alt="image-20230331112755659" style="zoom:50%;" />

<h3 id="剪枝处理"><a href="#剪枝处理" class="headerlink" title="剪枝处理"></a>剪枝处理</h3><h4 id="后剪枝"><a href="#后剪枝" class="headerlink" title="后剪枝"></a>后剪枝</h4><img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331112858597.webp" alt="image-20230331112858597" style="zoom:50%;" />

<h4 id="最小化子树的损失函数"><a href="#最小化子树的损失函数" class="headerlink" title="最小化子树的损失函数"></a>最小化子树的损失函数</h4><img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331112945689.webp" alt="image-20230331112945689" style="zoom:50%;" />

<h3 id="树学习算法优点"><a href="#树学习算法优点" class="headerlink" title="树学习算法优点"></a>树学习算法优点</h3><img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331111339376.webp" alt="image-20230331111339376" style="zoom:50%;" />

<h3 id="树学习算法缺点"><a href="#树学习算法缺点" class="headerlink" title="树学习算法缺点"></a>树学习算法缺点</h3><img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331111320360.webp" alt="image-20230331111320360" style="zoom:50%;" />

<h3 id="决策树的延申"><a href="#决策树的延申" class="headerlink" title="决策树的延申"></a>决策树的延申</h3><h4 id="深度学习时代的决策树"><a href="#深度学习时代的决策树" class="headerlink" title="深度学习时代的决策树"></a>深度学习时代的决策树</h4><img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331112339128.webp" alt="image-20230331112339128" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331112433745.webp" alt="image-20230331112433745" style="zoom:50%;" />

<h2 id="作业三：决策树"><a href="#作业三：决策树" class="headerlink" title="作业三：决策树"></a>作业三：决策树</h2><p>要求：给定药品数据集构造决策树，并用 Micro-F1 和 Macro-F1 分数进行验证集评估，预测测试集中的药品等级。</p>
<p>数据集说明：</p>
<ul>
<li>csv 数据集中包含有关药品的各种属性，文件中可能包含“脏”数据，药品等级分为5级（1~5）</li>
<li>训练集：6999条数据；验证集：1199条数据；测试集：1798条数据</li>
</ul>
<p><strong>代码展示</strong>：<a target="_blank" rel="noopener" href="https://github.com/Ling-Yuchen/ML-hw/blob/main/decision_tree.py">https://github.com/Ling-Yuchen/ML-hw/blob/main/decision_tree.py</a></p>
<h3 id="实验报告-1"><a href="#实验报告-1" class="headerlink" title="实验报告"></a>实验报告</h3><h4 id="数据的分析与处理"><a href="#数据的分析与处理" class="headerlink" title="数据的分析与处理"></a>数据的分析与处理</h4><p>给定数据集的每一个数据项的结构如下：</p>
<p><em>recordId | drugName | condition | reviewComment | date | usefulCount | sideEffects | rating</em></p>
<p>其中，<em>rating</em> 是需要进行学习的分类标记，根据常识进行判断，<em>recordId | drugName | date</em> 属于无效字段，<em>reviewComment</em> 包含大量自然语言，难以进行概括性的标准化的归纳处理，故初步将 <em>condition | usefulCount | sideEffects</em> 作为待定的分类主要属性依据</p>
<p>经过尝试，选取 <em>condition | sideEffects</em> 两个属性作为分类依据得到的验证数据效果最好，故在数据处理时，对每一条记录，只保留 <em>condition | sideEffects | rating</em> 三个属性的信息</p>
<p>具体处理流程见代码中的 <code>read_data</code> 方法</p>
<h4 id="决策树的设计原理"><a href="#决策树的设计原理" class="headerlink" title="决策树的设计原理"></a>决策树的设计原理</h4><ul>
<li>基于信息增益选择最优特征</li>
</ul>
<p>ID3 算法的核心思想是通过计算每个特征的信息增益，选择最优的特征来构建决策树。信息增益是用来衡量一个特征对分类结果的影响程度的指标，其值越大表示该特征对分类的贡献越大。在每个节点上，ID3 算法会计算每个特征的信息增益，并选择信息增益最大的特征作为节点的划分标准。</p>
<ul>
<li>递归地构建决策树</li>
</ul>
<p>在选择最优特征后，ID3算法会根据该特征的取值将当前节点分成多个子节点，并递归地对每个子节点进行相同的操作，直到所有叶子节点的类别相同或者无法再分。</p>
<ul>
<li>处理缺失值和连续值</li>
</ul>
<p>在处理缺失值时，ID3算法将缺失值看作一种特殊的取值，同时计算信息增益时不考虑缺失值所对应的分支。在处理连续值时，ID3算法需要对连续值进行离散化，将其分成若干个离散的取值，然后再计算信息增益。</p>
<p>（核心代码略）</p>
<h4 id="验证集评估结果"><a href="#验证集评估结果" class="headerlink" title="验证集评估结果"></a>验证集评估结果</h4><img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230321152606188.webp" alt="image-20230321152606188" style="zoom:70%;" />



<h1 id="集成学习"><a href="#集成学习" class="headerlink" title="集成学习"></a>集成学习</h1><h2 id="集成学习原理"><a href="#集成学习原理" class="headerlink" title="集成学习原理"></a>集成学习原理</h2><p><strong>原理</strong></p>
<ul>
<li>将多个个体学习器通过结合模块相结合，得到一个综合的输出</li>
<li>是一个预测模型的元方法</li>
</ul>
<p><strong>特点</strong></p>
<ul>
<li>多个分类器集成在一起，以提高分类准确率</li>
<li>由训练数据构建基分类器，然后根据预测结果进行投票</li>
<li>集成学习本身不是一种分类器，而是分类器结合方法</li>
<li>通常集成分类器性能会好于单个分类器</li>
</ul>
<p><strong>举例分析</strong></p>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331190833225.webp" alt="image-20230331190833225" style="zoom:50%;" />

<p><strong>Bias-Variance trade-off</strong></p>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331191106544.webp" alt="image-20230331191106544" style="zoom:50%;" />

<p><strong>核心问题</strong></p>
<ul>
<li>序列集成法<ul>
<li>利用基学习器之间的依赖关系，依次生成</li>
<li>减小偏差 bias</li>
</ul>
</li>
<li>并行集成法<ul>
<li>利用基学习器之间的独立关系，并行生成</li>
<li>减小方差 variance</li>
</ul>
</li>
</ul>
<p><em>Q1：如何训练每个学习器</em></p>
<p><em>Q2：如何结合每个学习器</em></p>
<p><strong>结合策略</strong></p>
<ul>
<li>平均法（回归问题）：简单平均 / 加权平均</li>
<li>投票法（分类问题）：绝对多数 / 相对多数 / 加权投票</li>
<li>学习法（Stacking）</li>
</ul>
<p><strong>多样性策略</strong>（学习基学习器）</p>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331191932431.webp" alt="image-20230331191932431" style="zoom:50%;" />

<h2 id="Bagging和随机森林"><a href="#Bagging和随机森林" class="headerlink" title="Bagging和随机森林"></a>Bagging和随机森林</h2><h3 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h3><p>有放回采样方法（统计上的目的是得到统计量分布以及置信区间）</p>
<h3 id="Bagging集成学习框架"><a href="#Bagging集成学习框架" class="headerlink" title="Bagging集成学习框架"></a>Bagging集成学习框架</h3><img src="http://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331192215585.webp" alt="image-20230331192215585" style="zoom:50%;" />

<h3 id="Bagging集成学习优点"><a href="#Bagging集成学习优点" class="headerlink" title="Bagging集成学习优点"></a>Bagging集成学习优点</h3><img src="http://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331192548627.webp" alt="image-20230331192548627" style="zoom:50%;" />

<h3 id="随机森林算法"><a href="#随机森林算法" class="headerlink" title="随机森林算法"></a>随机森林算法</h3><img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331192659991.webp" alt="image-20230331192659991" style="zoom:50%;" />

<img src="http://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331192726798.webp" alt="image-20230331192726798" style="zoom:50%;" />

<h2 id="Boosting和GBDT、XGboost"><a href="#Boosting和GBDT、XGboost" class="headerlink" title="Boosting和GBDT、XGboost"></a>Boosting和GBDT、XGboost</h2><h3 id="基本原理-1"><a href="#基本原理-1" class="headerlink" title="基本原理"></a>基本原理</h3><p>可以通过提升方法（Boosting）将弱学习器转为强学习器</p>
<h3 id="Boosting集成学习框架"><a href="#Boosting集成学习框架" class="headerlink" title="Boosting集成学习框架"></a>Boosting集成学习框架</h3><img src="http://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331192908421.webp" alt="image-20230331192908421" style="zoom:50%;" />

<h3 id="前向分步加法模型"><a href="#前向分步加法模型" class="headerlink" title="前向分步加法模型"></a>前向分步加法模型</h3><ul>
<li>加法模型及其目标函数</li>
</ul>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331213338867.webp" alt="image-20230331213338867" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331213356923.webp" alt="image-20230331213356923" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331213449009.webp" alt="image-20230331213449009" style="zoom:50%;" />

<ul>
<li>前向分布算法</li>
</ul>
<p>学习目标函数为加法模型，如果能够从前向后，每一步只学习一个基函数及其系数，逐步逼近要优化的总目标函数，就可以简化优化的复杂度</p>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331213646916.webp" alt="image-20230331213646916" style="zoom:50%;" />

<p><strong>残差</strong></p>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331213914616.webp" alt="image-20230331213914616" style="zoom:50%;" />

<h3 id="提升树"><a href="#提升树" class="headerlink" title="提升树"></a>提升树</h3><p>以决策树为基函数的提升方法成为提升树（Boosting Tree）</p>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331213817564.webp" alt="image-20230331213817564" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331213947622.webp" alt="image-20230331213947622" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331214019396.webp" alt="image-20230331214019396" style="zoom:50%;" />

<h3 id="梯度提升树-GBDT"><a href="#梯度提升树-GBDT" class="headerlink" title="梯度提升树 GBDT"></a>梯度提升树 GBDT</h3><img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331214109628.webp" alt="image-20230331214109628" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331214144745.webp" alt="image-20230331214144745" style="zoom:50%;" />

<p><strong>GBDT例子</strong></p>
<center>
    <img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331214333965.webp" alt="image-20230331214333965" style="zoom:30%;" />
    <img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331214359166.webp" alt="image-20230331214359166" style="zoom:30%;" />
</center>

<center>
    <img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331214422717.webp" alt="image-20230331214422717" style="zoom:33%;" />
    <img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331214446054.webp" alt="image-20230331214446054" style="zoom:33%;" />
</center>
### XGBoost

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331214712395.webp" alt="image-20230331214712395" style="zoom:50%;" />

<p><strong>树的复杂度</strong></p>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331215357782.webp" alt="image-20230331215357782" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331215422130.webp" alt="image-20230331215422130" style="zoom:50%;" />

<p><strong>目标函数</strong></p>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331215446907.webp" alt="image-20230331215446907" style="zoom:50%;" />

<p><strong>树的打分和分裂</strong></p>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331215511166.webp" alt="image-20230331215511166" style="zoom:50%;" />

<h2 id="Adaboost"><a href="#Adaboost" class="headerlink" title="Adaboost"></a>Adaboost</h2><p><strong>概率近似正确学习理论（PCA）</strong></p>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331222054923.webp" alt="image-20230331222054923" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331222122135.webp" alt="image-20230331222122135" style="zoom:50%;" />

<p><strong>Adaptive Boost</strong></p>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331222213356.webp" alt="image-20230331222213356" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331222246368.webp" alt="image-20230331222246368" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331222309457.webp" alt="image-20230331222309457" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331222333840.webp" alt="image-20230331222333840" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331222357634.webp" alt="image-20230331222357634" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331222421007.webp" alt="image-20230331222421007" style="zoom:50%;" />

<h2 id="学习法"><a href="#学习法" class="headerlink" title="学习法"></a>学习法</h2><h3 id="Stacking算法"><a href="#Stacking算法" class="headerlink" title="Stacking算法"></a>Stacking算法</h3><p><strong>算法原理</strong></p>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331221106823.webp" alt="image-20230331221106823" style="zoom:50%;" />

<p><strong>算法流程</strong></p>
<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331221144470.webp" alt="image-20230331221144470" style="zoom:50%;" />

<h3 id="Stacking"><a href="#Stacking" class="headerlink" title="Stacking"></a>Stacking</h3><img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331221249286.webp" alt="image-20230331221249286" style="zoom:50%;" />

<img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331221311224.webp" alt="image-20230331221311224" style="zoom:50%;" />

<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><img src="https://pictures-1312865652.cos.ap-nanjing.myqcloud.com/image-20230331221406391.webp" alt="image-20230331221406391" style="zoom:50%;" />

<h1 id="优化和搜索"><a href="#优化和搜索" class="headerlink" title="优化和搜索"></a>优化和搜索</h1><h1 id="维度约减"><a href="#维度约减" class="headerlink" title="维度约减"></a>维度约减</h1><h1 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h1><h1 id="演化学习"><a href="#演化学习" class="headerlink" title="演化学习"></a>演化学习</h1><h2 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h2><h2 id="遗传算法"><a href="#遗传算法" class="headerlink" title="遗传算法"></a>遗传算法</h2><h1 id="复习要点"><a href="#复习要点" class="headerlink" title="复习要点"></a>复习要点</h1><p><strong>误差反向传播</strong></p>
<p>以输入数据为一个二维坐标、输出数据为一个标签值为例，</p>
<p>记输入为 $(x_1,x_2)$，真实输出为 $y$，正确输出为 $y’$，使用的激活函数为 $\phi(x)$，第 $L$ 层第 $k$ 个神经元的权重为 $w^L_{k1}$,…, $w^L_{kn}$，对应接收的输入为 $a^{L-1}_1$,…, $a^{L-1}_n$，激活前输出为 $z^L_k$，激活后输出为 $a^L_k$</p>
<p>记第 $L$ 层第 $k$ 个神经元的 $delta$ 误差为 $\Delta_k^L$，则有：</p>
<ul>
<li>若第 $L$ 层为输出层，$D_k^L=\phi’(z_k^L)\times(y’-y)$</li>
<li>若第 $L$ 层为输出层，$D_k^L=\phi’(z_k^L)\times\sum_i(D_i^{L+1}w_{ik}^{L+1})$</li>
</ul>
<p>从而，$\Delta w_{ki}^L=\alpha\times D_k^L\times a^{L-1}_i$</p>
<p>更新权值：$w_{ki}^L=w_{ki}^L-\Delta w_{ki}^L$</p>
<p><strong>ID3 决策树算法</strong></p>
<p>以实例标签为布尔值为例，</p>
<p>信息熵计算：$Ent(S)=-\sum_i\space p_ilog_2p_i$，其中 $S$ 为样本集合，$p_i$ 为标签 $i$ 出现的概率</p>
<p>信息增益计算：$Gain(S,A)=Ent(S)-\sum_v\frac{|S_v|}{|S|}Ent(S_v)$，其中 $A$ 为作为分割标准的属性，$v$ 为该属性的取值</p>
<p>递归构建决策树：</p>
<ul>
<li>若当前样本集中所有实例的标签相同，则 root 赋值为改标签，返回</li>
<li>否则，若没有可继续分割样本集的属性，则投票选出样本集中最多的标签，root 赋值为该标签，返回</li>
<li>否则，选取信息增益最大的属性，作为 root 的决策属性，以该属性的值为依据，分割样本集为若干子样本集</li>
<li>若子样本集中有空集，则增加一个叶节点，以样本集中出现最多的标签作为其标签</li>
<li>否则在该分支下依据子样本集和子属性集增加对应的子树</li>
</ul>
<p><strong>MDP 模型</strong></p>
<p>$V^{\pi}(s)$：从 $s$ 状态出发，采用 $π$ 策略，所获得的期望返回值</p>
<p>$Q^{\pi}(s,a)$：从 $s$ 状态出发， 采用 $a$动作，继而采用 $π$ 策略，所获得的期望返回值</p>
<p>$V^{\pi}(s)=Q^{\pi}(s,\pi(s,a))$</p>
<ul>
<li><p>策略评估：计算 $V^{\pi}(s)=E(R(s,\pi(s)))+\gamma\space\sum_{s’}\space\delta(s,\pi(s),s’)V^{\pi}(s’)$，</p>
</li>
<li><p>最优控制：计算 $Q^{\pi}(s,a)=E(R(s,a))+\gamma\space\sum_{s’}\space\delta(s,a,s’)V^{\pi}(s’)$</p>
<ul>
<li>修改策略：$\pi(s)=argmax_a(Q^{\pi}(s,a))$（贪心策略） 或（$\epsilon$-贪心策略）</li>
</ul>
</li>
</ul>
<p><strong>KNN 算法</strong></p>
<p>k近邻分类：</p>
<ul>
<li>计算样本 $x$ 和所有训练样本 $x_i$ 之间的距离并排序</li>
<li>选取 k 个最近的训练样本，采用投票法，将近邻样本中数量最多的类标签分配给 $x$</li>
</ul>
<p>k近邻回归：</p>
<ul>
<li>计算样本 $x$ 和所有训练样本 $x_i$ 之间的距离并排序</li>
<li>选取 k 个最近的训练样本，以距离值倒数作为权重，将近邻的标签值加权平均，作为 $x$ 的预测标签值</li>
</ul>
<p>训练阶段时间复杂度为 0，测试阶段时间复杂度：$O(nd+nlogk)$</p>
<p><strong>LDA 算法基本流程</strong></p>
<ul>
<li>计算每个类的均值向量 $\mu_c$ 和全局样本的均值向量 $\mu$</li>
<li>计算类内散度矩阵 $S_w=\sum_{class\space c}p_c\sum_{j\in c}(x_j-\mu_c)(x_j-\mu_c)^T$</li>
<li>计算类间散度矩阵 $S_b=\sum_{class\space c}(\mu_c-\mu)(\mu_c-\mu)^T$</li>
<li>计算矩阵 $S_w^{-1}S_b$ 并计算其特征值和特征向量</li>
<li>根据需要取若干最大特征值对应的特征向量组成投影矩阵</li>
</ul>
<p><strong>PCA 算法基本流程</strong></p>
<ul>
<li>计算样本的均值向量 $\mu$，并将样本去中心化：$x_i=x_i-\mu$</li>
<li>计算样本的协方差矩阵 $C=\sum_ix_ix_i^T=XX^T$</li>
<li>计算协方差矩阵 $C$ 的特征值和特征向量</li>
<li>根据需要取若干最大特征值对应的特征向量组成投影矩阵</li>
</ul>
<p><strong>牛顿法迭代优化算法流程</strong></p>
<p>根据当前迭代点 $x_k$ 计算雅可比矩阵和海森矩阵：$J_k=J(f(x))|<em>{x=x_k}$，$H_k=H(f(x))|</em>{x=x_k}$ </p>
<p>计算 $p_k=-H_k^{-1}J_k^T$，得到新的迭代点 $x_{k+1}=x_k+p_k$，直至 $||J_k||\le\epsilon$ 为止</p>
<p><strong>最小二乘法迭代优化算法流程</strong></p>
<p>目标函数的形式为：$f(x)=\frac{1}{2}\sum_{j=1}^{m}r^2_j(x)=\frac{1}{2}||r(x)||_2^2$</p>
<p>根据当前迭代点 $x_k$，有：$x_{k+1}=-(J^T(x)J(x))^{-1}J^T(x)r(x)|_{x=x_k}$，其中 $J(x)$ 为函数 $r(x)$ 的雅可比矩阵</p>

        </div>

        
            <section class="post-copyright">
                
                    <p class="copyright-item">
                        <span>Author:</span>
                        <span>Yoson Ling</span>
                    </p>
                
                
                
                    <p class="copyright-item">
                        <span>License:</span>
                        <span>Copyright (c) 2019 <a target="_blank" rel="noopener" href="http://creativecommons.org/licenses/by-nc/4.0/">CC-BY-NC-4.0</a> LICENSE</span>
                    </p>
                
                

            </section>
        
        <section class="post-tags">
            <div>
                <span>Tag(s):</span>
                <span class="tag">
                    
                    
                        <a href="/tags/NJU/"># NJU</a>
                    
                        <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"># 机器学习</a>
                    
                        
                </span>
            </div>
            <div>
                <a href="javascript:window.history.back();">back</a>
                <span>· </span>
                <a href="/">home</a>
            </div>
        </section>
        <section class="post-nav">
            
                <a class="prev" rel="prev" href="/2023/02/15/RoveTest/">RoveTest（ing）</a>
            
            
            <a class="next" rel="next" href="/2022/12/31/NJU-Business-Intelligence-Review/">南京大学《商务智能》复习笔记</a>
            
        </section>


    </article>
</div>

            </div>
            <footer id="footer" class="footer">
    <div class="copyright">
        <span>© Yoson Ling | Powered by <a href="https://hexo.io" target="_blank">Hexo</a> & <a href="https://github.com/Siricee/hexo-theme-Chic" target="_blank">Chic</a></span>
    </div>
</footer>

    </div>
</body>

</html>